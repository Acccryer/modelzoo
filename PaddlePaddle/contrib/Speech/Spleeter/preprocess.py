import csv
import json
import os

import soundfile as sf
from tqdm import tqdm
from util import HParams
from run_scripts.argument import config

cmd = config()

mode = cmd.mode
device = ",".join([str(i) for i in range(cmd.nproc_per_node)])
assert cmd.model_name == '2instruments',"Only 2 instruments are supported."

# 设置环境变量
os.environ["CUSTOM_DEVICE_BLACK_LIST"] = "conv2d,conv2d_grad"
os.environ["SDAA_VISIBLE_DEVICES"] = device
os.environ["PADDLE_XCCL_BACKEND"] = "sdaa"

params = {
    ### Dataset ###
    'margin': cmd.margin,
    'chunk_duration': cmd.chunk_duration,
    'sample_rate': cmd.sample_rate,
    'frame_length': cmd.frame_length,
    'frame_step': cmd.frame_step,
    'T': cmd.T,
    'F': cmd.F,
    'n_chunks_per_song': cmd.n_chunks_per_song,
    'train_manifest': cmd.train_manifest, # Manifest generated by preprocess.py
    'train_dataset' : r'dataset',

    ### Train ###
    'epochs': cmd.epoch,
    'batch_size': cmd.batch_size if mode != "process_data" else 1,
    'optimizer': cmd.optimizer,
    'loss': cmd.loss,
    'momentum': cmd.momentum,
    'dampening': cmd.dampening,
    'lr': cmd.lr,
    'lr_decay': cmd.lr_decay,
    'wd': cmd.wd,
    'model_dir': cmd.model_dir,
    'load_optimizer': cmd.load_optimizer,
    'start': cmd.start,
    'load_model': cmd.load_model, 
    'num_instruments': ['vocal', 'instrumental'],
    'seed': cmd.seed,
    'keep_ckpt' : cmd.keep_ckpt,
    'trainer' : cmd.trainer,
    'clean_logs' : cmd.clean_logs,
    'amp' : cmd.amp
}

hps = HParams(**params)
train_manifest = hps.train_manifest
train_dataset = hps.train_dataset

'''
(Dir Structure)
dataset
  |-song1
     |- mixture.wav
     |- vocals.wav
     |- instrumental.wav
  |-song2
  	 |- mixture.wav
	 |- vocals.wav
	 |- instrumental.wav

'''

'''
def read_manifest(dataset, start = 0):
    rows = []
    with open(MANIFEST_DIR.format(dataset), 'r') as f:
        reader = csv.reader(f)
        for sid, aid, filename, duration, samplerate in reader:
            rows.append([int(sid) + start, aid, filename, duration, samplerate])
            n_speakers = int(sid) + 1
    return n_speakers, rows
'''

def save_manifest(train_manifest, rows):
    rows.sort()
    with open(train_manifest, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerows(rows)



def create_manifest(train_dataset):
    n_speakers = 0
    log = []
    
    for song in tqdm(os.listdir(train_dataset), desc = None):
        song_dir = os.path.join(train_dataset, song)
        if os.path.isdir(song_dir) == False:
            continue
        for audio in os.listdir(song_dir):
            audio_path = os.path.join(song_dir, audio)
            
            if 'mix' in audio:
                mix_path = audio_path
            elif 'vocal' in audio:
                vocal_path = audio_path
            elif 'instru' in audio:
                instru_path = audio_path
            else:
              continue

            
            filename = os.path.join(song_dir, audio)
            info = sf.info(filename)
            duration = info.duration
            samplerate = info.samplerate
        log.append((mix_path, vocal_path, instru_path, duration, samplerate))
    save_manifest(train_manifest, log)

'''
def merge_manifest(datasets, dataset):
    rows = []
    n = len(datasets)
    start = 0
    for i in range(n):
        n_speakers, temp = read_manifest(datasets[i], start = start)
        rows.extend(temp)
        start += n_speakers
    with open(MANIFEST_DIR.format(dataset), 'w') as f:
        writer = csv.writer(f)
        writer.writerows(rows)

def cal_eer(y_true, y_pred):
    fpr, tpr, thresholds= roc_curve(y_true, y_pred, pos_label = 1)
    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)
    thresh = interp1d(fpr, thresholds)(eer)
    return eer, thresh

'''

if __name__ == '__main__':
    create_manifest(train_dataset)
