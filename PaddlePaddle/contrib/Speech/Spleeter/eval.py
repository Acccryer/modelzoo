#encoding=utf-8
from typing import List

import os
import sys
import time
import json
import random
import librosa
import museval
import numpy as np
import multiprocessing
from multiprocessing import Manager
from collections import OrderedDict
from tqdm import tqdm
from glob import glob
from musdb.audio_classes import Track
from musdb.audio_classes import MultiTrack
from run_scripts.argument import config

eval_targets = ["vocals", "accompaniment"]
debug = False
length: List[int] = []
time_usage: List[int] = []
models: List = []
dataset: List[str] = None
dataset_path: str = None
cmd = None

class HParams():
  def __init__(self, **kwargs):
    for k, v in kwargs.items():
      if type(v) == dict:
        v = HParams(**v)
      self[k] = v

  def keys(self):
    return self.__dict__.keys()

  def items(self):
    return self.__dict__.items()

  def values(self):
    return self.__dict__.values()

  def __len__(self):
    return len(self.__dict__)

  def __getitem__(self, key):
    return getattr(self, key)

  def __setitem__(self, key, value):
    return setattr(self, key, value)

  def __contains__(self, key):
    return key in self.__dict__

  def __repr__(self):
    return self.__dict__.__repr__()

def deDaoZuiXinMoXing(params:HParams) -> list:
    moXinLuJing:str = params.model_dir
    moXing = list()
    epoch_list = list()
    for i in range(len(params['num_instruments'])):
        for j in os.listdir(moXinLuJing):
            if len(j.split('_')) == 3 and j.split('_')[0] == 'net' and j.split('_')[1] == params['num_instruments'][i] and j.split('_')[-1].endswith('.pdparams'):
                try:
                    model_epoch = int(j.split('_')[-1].replace('.pdparams',''))
                    epoch_list.append(model_epoch)

                except Exception as e:
                    continue
    epoch_list.sort()
    try:
        latest_epoch = epoch_list[-1]
    except:
        return [None,None]
    for meiGeYueQi in params['num_instruments']:
        if not os.path.isfile(os.path.join(params.model_dir, f'net_{meiGeYueQi}_{latest_epoch}.pdparams')):
            return [None, None]
        moXing.append(os.path.join(params.model_dir, f'net_{meiGeYueQi}_{latest_epoch}.pdparams'))

    return moXing

# 根据task id和卡的数量自动选择使用的设备
def worker(task_id, world_size, queue):
    global dataset, dataset_path, cmd

    params = {
        ### Dataset ###
        'margin': cmd.margin,
        'chunk_duration': cmd.chunk_duration,
        'sample_rate': cmd.sample_rate,
        'frame_length': cmd.frame_length,
        'frame_step': cmd.frame_step,
        'T': cmd.T,
        'F': cmd.F,
        'n_chunks_per_song': cmd.n_chunks_per_song,
        'train_manifest': cmd.train_manifest, # Manifest generated by preprocess.py
        'train_dataset' : r'dataset',
    
        ### Train ###
        'epochs': cmd.epoch,
        'batch_size': cmd.batch_size if cmd.mode != "process_data" else 1,
        'optimizer': cmd.optimizer,
        'loss': cmd.loss,
        'momentum': cmd.momentum,
        'dampening': cmd.dampening,
        'lr': cmd.lr,
        'lr_decay': cmd.lr_decay,
        'wd': cmd.wd,
        'model_dir': cmd.model_dir,
        'load_optimizer': cmd.load_optimizer,
        'start': cmd.start,
        'load_model': cmd.load_model, 
        'num_instruments': ['vocal', 'instrumental'],
        'seed': cmd.seed,
        'keep_ckpt' : cmd.keep_ckpt,
        'trainer' : cmd.trainer,
        'clean_logs' : cmd.clean_logs,
        'amp' : cmd.amp
    }
    
    device_dataset = []
    device_id = task_id % world_size
    # 每个device分配对应的数据集
    device_dataset.append(dataset[task_id])

    # 初始化模型到对应的卡上面
    import paddle
    import paddle.audio as paddleaudio
    import paddle_sdaa
    from separator import Separator
    paddle.set_device("sdaa:{}".format(device_id))
    sep = Separator(resume = deDaoZuiXinMoXing(HParams(**params)))
    sep.eval()
    paddle.set_device("cpu")

    print(deDaoZuiXinMoXing(HParams(**params)))

    for name in device_dataset:
        i = os.path.join(dataset_path,name)
        vocal_track = Track(path=(vocal_path:=os.path.join(i,"vocal.wav")), stem_id=0,
                            is_wav=True, subset="test", sample_rate=(sr:=cmd.sample_rate))
        accompaniment_track = Track(path=(accompaniment_path:=os.path.join(i,"instrumental.wav")), stem_id=0,
                            is_wav=True, subset="test", sample_rate=(sr:=cmd.sample_rate))
        targets = OrderedDict([("vocals",vocal_track),("accompaniment",accompaniment_track)])
        track = MultiTrack(path=vocal_path, name=name, targets=targets, sample_rate=sr, stem_id=0, subset="test")

        # 第0个轨道输出的是人声轨，第1个轨道输出的是伴奏轨道
        # 对输入音频进行分离处理
        shuRu = list(paddleaudio.load(os.path.join(i,"mixture.wav"))) # 加载输入
        data = paddle.to_tensor(librosa.resample(shuRu[0].numpy(), shuRu[1], 44100)) # 数据重采样

        # 记录数据长度
        length.append(data.shape[1])

        # 进行推理
        start_time = time.time()
        paddle.set_device("sdaa:{}".format(device_id))
        print("卡{}推理{}".format(device_id,os.path.join(i,"mixture.wav")))
        shuChu = sep(data,44100)
        time_usage.append(time.time() - start_time)

        # 计算指标
        v = shuChu[0].T.numpy() # shape: [channel, num_samples] -> [num_samples, channel]
        a = shuChu[1].T.numpy() # type: paddle.Tensor -> numpy.NDarray
        result = museval.eval_mus_track(track,
                                        dict(vocals=v,
                                             accompaniment=a,
                                            ),
                                        "test_metrics",
                                       )
        del shuChu,sep,data,shuRu
        queue.put(1)  # 放入一个标记，表示任务完成
        
 
def show_progress(total, queue):
    with tqdm(total=total) as pbar:
        while queue.qsize() < total:
            time.sleep(1)  # 避免CPU占用过高
            pbar.update(queue.qsize() - pbar.n)

def get_duration_mp3_and_wav(file_path):
     """
     获取mp3/wav音频文件时长，返回单位为秒
     """
     duration = librosa.get_duration(filename=file_path)
     return duration
                        
def main() -> None:
    global dataset, models, dataset_path, cmd
    ori_dataset = os.listdir((dataset_path:=(cmd:=config()).train_dataset))
    assert len(ori_dataset),"the length of dataset cannot be 0!"
    # 随机抽样
    random.seed(cmd.seed)
    random.shuffle(ori_dataset)

    # 设置测试数据集
    count = int(len(ori_dataset)*(1/10))
    pointer = 0
    dataset = list()
    while count!=0:
        if get_duration_mp3_and_wav(os.path.join(dataset_path,ori_dataset[pointer],"mixture.wav"))/60 < 6: # 只有小于6分钟的音频才不会oom
            dataset.append(ori_dataset[pointer])
            count -= 1
        pointer += 1 # 无论是否添加成功指针都++

    manager = Manager()
    queue = manager.Queue()
    num_tasks = len(dataset)
    num_worker = cmd.nproc_per_node
    
    # 创建进度条显示进程
    progress_process = multiprocessing.Process(target=show_progress, args=(num_tasks, queue))
    progress_process.start()
    
    # 创建工作进程
    processes = []
    for task_id in range(num_tasks):
        # 每个任务创建一个进程去执行，执行完就销毁进程
        process = multiprocessing.Process(target=worker, args=(task_id, num_worker, queue))
        process.start()
        processes.append(process)

        #如果启动的进程的数量和卡的数量一致，那么就等待所有进程执行完毕再继续新建进程
        if len(processes) == num_worker:
            # 等待所有工作进程完成
            for process in processes:
                process.join()
                
            # 所有进程执行完之后
            processes.clear()
    else:
        for process in processes:
            process.join()
            
        # 所有进程执行完之后
        processes.clear()

    #queue.put(None) # 通知进度条进程停止
    progress_process.join()

    metrics1 = ['SDR', 'SAR', 'SIR', 'ISR']
    metrics_root = 'test_metrics'
    eval_targets = ["vocals", "accompaniment"]
    songs = glob(os.path.join(metrics_root, "test", '*.json'))
    metrics = {instrument: {k: [] for k in metrics1} for instrument in eval_targets}
    for song in songs:
        with open(song, 'r') as stream:
            data = json.load(stream)
        for target in data['targets']:
            instrument = target['name']
            for metric in metrics1:
                sdr_med = np.median([
                    frame['metrics'][metric]
                    for frame in target['frames']
                    if not np.isnan(frame['metrics'][metric])])
                metrics[instrument][metric].append(sdr_med)
    
    for instrument, metric in metrics.items():
        print(instrument)
        for key, value in metric.items():
            print(key, f'{np.median(value):.3f}')

# 单卡
def infer() -> None:
    dataset = os.listdir((dataset_path:=(cmd:=config()).train_dataset))
    assert len(dataset),"the length of dataset cannot be 0!"
    # 随机抽样
    random.seed(cmd.seed)
    random.shuffle(dataset)
    dataset = dataset[:int(len(dataset)*(1/10))]
    paddle.set_device("sdaa:{}".format(device_id))
    sep = Separator(resume = ['model/net_vocal_0.pdparams', 'model/net_instrumental_0.pdparams'])
    sep.eval()
    paddle.set_device("cpu")
    for name in tqdm(dataset):
        i = os.path.join(dataset_path,name)
        vocal_track = Track(path=(vocal_path:=os.path.join(i,"vocal.wav")), stem_id=0,
                            is_wav=True, subset="test", sample_rate=(sr:=cmd.sample_rate))
        accompaniment_track = Track(path=(accompaniment_path:=os.path.join(i,"instrumental.wav")), stem_id=0,
                            is_wav=True, subset="test", sample_rate=(sr:=cmd.sample_rate))
        targets = OrderedDict([("vocals",vocal_track),("accompaniment",accompaniment_track)])
        track = MultiTrack(path=vocal_path, name=name, targets=targets, sample_rate=sr, stem_id=0, subset="test")

        # 第0个轨道输出的是人声轨，第1个轨道输出的是伴奏轨道
        # 对输入音频进行分离处理
        shuRu = list(paddleaudio.load(os.path.join(i,"mixture.wav"))) # 加载输入
        data = paddle.to_tensor(librosa.resample(shuRu[0].numpy(), shuRu[1], 44100)) # 数据重采样

        # 记录数据长度
        length.append(data.shape[1])

        # 进行推理
        start_time = time.time()
        paddle.set_device("sdaa:{}".format(device_id))
        shuChu = sep(data,44100)
        time_usage.append(time.time() - start_time)

        # 计算指标
        v = shuChu[0].T.numpy() # shape: [channel, num_samples] -> [num_samples, channel]
        a = shuChu[1].T.numpy() # type: paddle.Tensor -> numpy.NDarray
        result = museval.eval_mus_track(track,
                                        dict(vocals=v,
                                             accompaniment=a,
                                            ),
                                        "test_output",
                                       )
        print(result)
        print(type(result))

        if debug:
            break

if __name__ == "__main__":
    main()